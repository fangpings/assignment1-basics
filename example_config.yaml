# Example configuration file
# Model configuration
vocab_size: 10000
context_length: 256
d_model: 512
num_layers: 4
num_heads: 16
d_ff: 1344
rope_theta: 10000

# Optimizer configuration
max_learning_rate: 0.003
min_learning_rate: 0.0003
warmup_iters: 500
cosine_cycle_iters: 5000
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
max_l2_norm: 1.0

# Trainer configuration
train_path: "data/tiny_stories/tokenized_train.npy"
validation_path: "data/tiny_stories/tokenized_valid.npy"
checkpoint_dir: "checkpoints/"
max_iteration: 5000
batch_size: 32
checkpoint_steps: 10000
eval_steps: 100
log_dir: "logs/"
experiment_name: "tinystories_lr_3e-2"