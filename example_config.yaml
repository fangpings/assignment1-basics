# Example configuration file
# Model configuration
vocab_size: 10000
context_length: 256
d_model: 512
num_layers: 4
num_heads: 16
d_ff: 1344
rope_theta: 10000

# Optimizer configuration
max_learning_rate: 0.0003
min_learning_rate: 0.00003
warmup_iters: 2000
cosine_cycle_iters: 38000
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
max_l2_norm: 1.0

# Trainer configuration
train_path: "data/tiny_stories/tokenized_train.npy"
validation_path: "data/tiny_stories/tokenized_valid.npy"
checkpoint_dir: "checkpoints/"
max_iteration: 40000
batch_size: 32
checkpoint_steps: 5
eval_steps: 500
log_dir: "logs/"